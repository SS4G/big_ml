{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('abc').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = sns.load_dataset(\"tips\")\n",
    "tips_df[\"id\"] = tips_df.index\n",
    "tips_df = shuffle(tips_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从pandas 的dataFrame 创建spark DataFrame\n",
    "spark_df = spark.createDataFrame(tips_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+------+------+---+------+----+---+\n",
      "|index|total_bill| tip|   sex|smoker|day|  time|size| id|\n",
      "+-----+----------+----+------+------+---+------+----+---+\n",
      "|   13|     18.43| 3.0|  Male|    No|Sun|Dinner|   4| 13|\n",
      "|   16|     10.33|1.67|Female|    No|Sun|Dinner|   3| 16|\n",
      "|    7|     26.88|3.12|  Male|    No|Sun|Dinner|   4|  7|\n",
      "+-----+----------+----+------+------+---+------+----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark 对比 hive 操作\n",
    "**pyspark的基本思想就是 lazy 操作 先用一系列的api来描述对数据的操作 后面spark会对操作进行转化**\n",
    "\n",
    "**pyspark 可以将DataFrame 直接使用 df.createOrRepaceTempView(\"table_name\") 转化为 hive table   \n",
    "然后使用 spark.sql(<sql_string>) 来操作 返回新的DataFrame**   \n",
    "\n",
    "\n",
    "**此处的对比仅包括读操作 hive大部分情况下是只读的**\n",
    "\n",
    "\n",
    "\n",
    "pyspark 的操作中 有两种获取列的方式 \n",
    "一是使用字符串 类似于pandas df[\"col1\"] \n",
    "二是df.col1 直接将列名作为一个对象 来获取   \n",
    "三是 from pyspark.sql.functions import col   col(\"col1\") 来获取\n",
    "\n",
    "和pandas的列可以直接显示不同 pyspark的 的列是 一个列对象 只是一个符号 只有在spark具体运行的过程中才会被解析\n",
    "\n",
    "列对象运算后的结果 或者经过udf 返回的结果仍然是列对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select\n",
    "```SQL\n",
    "select \n",
    "    tip,\n",
    "    sex\n",
    "from tips_df\n",
    "```\n",
    "select 参数就是选出列的子集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| tip|   sex|\n",
      "+----+------+\n",
      "|1.01|Female|\n",
      "|1.66|  Male|\n",
      "+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#直接使用字符春做参数 \n",
    "df1 = spark_df.select(\"tip\", \"sex\")\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| tip|    sx|\n",
      "+----+------+\n",
      "|1.01|Female|\n",
      "|1.66|  Male|\n",
      "+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#使用字符串 + spark.sql.functions.col 获取列对象做参数\n",
    "df2 = spark_df.select(F.col(\"tip\"), F.col(\"sex\").alias(\"sx\"))\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| tip|    sx|\n",
      "+----+------+\n",
      "|1.01|Female|\n",
      "|1.66|  Male|\n",
      "+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 对象成员 的方式获取列对象 \n",
    "df3 = spark_df.select(spark_df.tip, spark_df.sex.alias(\"sx\"))\n",
    "df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| tip|    sx|\n",
      "+----+------+\n",
      "|1.01|Female|\n",
      "|1.66|  Male|\n",
      "+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用索引的方式获取列队对象 也可以把列对象直接放在一个list 里面传入 \n",
    "df4 = spark_df.select([spark_df[\"tip\"], spark_df.sex.alias(\"sx\")])\n",
    "df4.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### where \n",
    "```SQL\n",
    "select \n",
    "    tip,\n",
    "    sex\n",
    "from tips_df\n",
    "where smoker != 'No' and (size > 2 or sex = 'Female')\n",
    "```\n",
    "pypark的 where(filter的别名) 使用bool的列对象作为输入 使用 & | ~ 来对bool列对象进行运算  \n",
    "或者直接用类似于 hive的 condition 字符串来进行运算   \n",
    "下面列出了四种where的调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame 可以直接count查看行数 \n",
    "# 但是并不是 sql中的那种对某一列进行count的函数 \n",
    "spark_where_df.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     38.01| 3.0|  Male|   Yes|Sat|Dinner|   4|\n",
      "|     18.29|3.76|  Male|   Yes|Sat|Dinner|   4|\n",
      "|      3.07| 1.0|Female|   Yes|Sat|Dinner|   1|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 df.col 的方式来获取列对象 \n",
    "# 注意 & 的优先级 要给条件语句加上括号才行！！！！\n",
    "spark_where_df1 = spark_df.where((spark_df.smoker != 'No') & ((spark_df.size > 2) | (spark_df.sex == 'Female')))\n",
    "spark_where_df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     38.01| 3.0|  Male|   Yes|Sat|Dinner|   4|\n",
      "|     18.29|3.76|  Male|   Yes|Sat|Dinner|   4|\n",
      "|      3.07| 1.0|Female|   Yes|Sat|Dinner|   1|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和上面完成相同的功能\n",
    "spark_where_df2 = spark_df.where((F.col(\"smoker\") != 'No') & ((F.col(\"size\") > 2) | (F.col(\"sex\") == 'Female')))\n",
    "spark_where_df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     38.01| 3.0|  Male|   Yes|Sat|Dinner|   4|\n",
      "|     18.29|3.76|  Male|   Yes|Sat|Dinner|   4|\n",
      "|      3.07| 1.0|Female|   Yes|Sat|Dinner|   1|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和上面完成相同的功能\n",
    "spark_where_df3 = spark_df.where((spark_df[\"smoker\"] != 'No') & ((spark_df[\"size\"] > 2) | (spark_df[\"sex\"] == 'Female')))\n",
    "spark_where_df3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     38.01| 3.0|  Male|   Yes|Sat|Dinner|   4|\n",
      "|     18.29|3.76|  Male|   Yes|Sat|Dinner|   4|\n",
      "|      3.07| 1.0|Female|   Yes|Sat|Dinner|   1|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和上面完成相同的功能 \n",
    "spark_where_df4 = spark_df.where(\"smoker != 'No' and (size > 2 or sex = 'Female')\")\n",
    "spark_where_df4.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum count avg distinct\n",
    "```SQL\n",
    "select\n",
    "    count(sex) as sex_cnt,\n",
    "    count(day) as day_cnt,\n",
    "    count(distinct sex) as sex_distinct_cnt,\n",
    "    sum(tip) as tip_sum,\n",
    "    sum(size) as size_sum\n",
    "from tips_df\n",
    "```\n",
    "如果对 pyspark 的列 进行操作 那么就需要 借助 spark.sql.functions 里面的sum count countDistinct 来实现 需要描述输出的列  \n",
    "推荐使用alias 对列名进行修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------------+-------+--------+\n",
      "|sex_cnt|day_cnt|sex_distinct_cnt|tip_sum|size_sum|\n",
      "+-------+-------+----------------+-------+--------+\n",
      "|    244|    244|               2| 731.58|     627|\n",
      "+-------+-------+----------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggr_df = spark_df.select(F.count(F.col(\"sex\")).alias(\"sex_cnt\"), \n",
    "                          F.count(F.col(\"day\")).alias(\"day_cnt\"), \n",
    "                          F.countDistinct(F.col(\"sex\")).alias(\"sex_distinct_cnt\"),\n",
    "                          F.sum(F.col(\"tip\")).alias(\"tip_sum\"),\n",
    "                          F.sum(F.col(\"size\")).alias(\"size_sum\")\n",
    "                         )\n",
    "aggr_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### group by\n",
    "```SQL\n",
    "select\n",
    "    sex,\n",
    "    smoker,\n",
    "    sum(tip) as tip_sum,\n",
    "    sum(size) as size_sum,\n",
    "    count(distinct day) as day_dis_cnt\n",
    "from tips_df\n",
    "group by sex, smoker\n",
    "```\n",
    "spark的DataFrame 使用groupBy（等价于 groupby） 后 变为一个 groupedData 对象   \n",
    "数据对象转化过程   \n",
    "DataFrame -> groupby() -> groupedData -> agg() -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------------+--------+-----------+\n",
      "|   sex|smoker|          tip_sum|size_sum|day_dis_cnt|\n",
      "+------+------+-----------------+--------+-----------+\n",
      "|  Male|    No|            302.0|     263|          4|\n",
      "|  Male|   Yes|           183.07|     150|          4|\n",
      "|Female|    No|           149.77|     140|          4|\n",
      "|Female|   Yes|96.74000000000001|      74|          4|\n",
      "+------+------+-----------------+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame -> groupby() -> groupedData -> agg() -> DataFrame\n",
    "agg_df = spark_df.groupBy(\"sex\", F.col(\"smoker\")).agg(F.sum(\"tip\").alias(\"tip_sum\"), F.sum(\"size\").alias(\"size_sum\"), F.countDistinct(\"day\").alias(\"day_dis_cnt\"))\n",
    "agg_df.show()\n",
    "type(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+------------------+----------+\n",
      "|   sex|smoker|sum(size)|          sum(tip)|count(day)|\n",
      "+------+------+---------+------------------+----------+\n",
      "|  Male|    No|      263|             302.0|        97|\n",
      "|  Male|   Yes|      150|183.07000000000002|        60|\n",
      "|Female|    No|      140|149.76999999999998|        54|\n",
      "|Female|   Yes|       74|             96.74|        33|\n",
      "+------+------+---------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用字典参数 只能是 key 和 value都为string的字典 而且 无法重命名列 比较受限\n",
    "agg_df = spark_df.groupBy(\"sex\", F.col(\"smoker\")).agg({\"tip\": \"sum\", \"size\": \"sum\", \"day\": \"count\"})\n",
    "agg_df.show()\n",
    "type(agg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order by\n",
    "```SQL\n",
    "select\n",
    "    sex,\n",
    "    smoker, \n",
    "    tip,\n",
    "    size\n",
    "from tips_df\n",
    "order by tips, size desc\n",
    "```\n",
    "order by 中的列要在select 中出现 order by 是最后执行的语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+----+\n",
      "|   sex|smoker|tip|size|\n",
      "+------+------+---+----+\n",
      "|  Male|   Yes|1.0|   2|\n",
      "|Female|   Yes|1.0|   2|\n",
      "|Female|    No|1.0|   1|\n",
      "+------+------+---+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 列对象的 desc方法 返回 一个反向排序的列对象 \n",
    "agg_df = spark_df.select(\"sex\", \"smoker\", \"tip\", \"size\").orderBy(F.col(\"tip\"), F.col(\"size\").desc())\n",
    "agg_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+----+\n",
      "|   sex|smoker|tip|size|\n",
      "+------+------+---+----+\n",
      "|  Male|   Yes|1.0|   2|\n",
      "|Female|   Yes|1.0|   2|\n",
      "|Female|    No|1.0|   1|\n",
      "+------+------+---+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  或者使用 ascending方法对\n",
    "agg_df = spark_df.select(\"sex\", \"smoker\", \"tip\", \"size\").orderBy(F.col(\"tip\"), F.col(\"size\"), ascending=[True, False])\n",
    "agg_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join union 测试\n",
    "#### join\n",
    "```SQL\n",
    "select\n",
    "    left_df.left_id as id,\n",
    "    total_bill,\n",
    "    smoker\n",
    "from left_df join right_df on left_df.left_id = right_df.right_id\n",
    "where size = 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+------+\n",
      "| id|total_bill| tip|   sex|\n",
      "+---+----------+----+------+\n",
      "|  0|     16.99|1.01|Female|\n",
      "|  1|     10.34|1.66|  Male|\n",
      "+---+----------+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_df = spark_df\n",
    "left_df =  all_df[[\"id\", \"total_bill\", \"tip\", \"sex\"]]\n",
    "left_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----+\n",
      "| id|smoker|day|  time|size|\n",
      "+---+------+---+------+----+\n",
      "|  0|    No|Sun|Dinner|   2|\n",
      "|  1|    No|Sun|Dinner|   3|\n",
      "+---+------+---+------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right_df = all_df[[\"id\", \"smoker\", \"day\", \"time\", \"size\"]]\n",
    "right_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+---+------+---+------+----+\n",
      "|total_bill|tip|   sex| id|smoker|day|  time|size|\n",
      "+----------+---+------+---+------+---+------+----+\n",
      "|     13.37|2.0|  Male| 26|    No|Sat|Dinner|   2|\n",
      "|     19.65|3.0|Female| 29|    No|Sat|Dinner|   2|\n",
      "+----------+---+------+---+------+---+------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 合并的 on参数 是一个 bool列对象  drop 去掉多余的id列 \n",
    "merged_df = left_df.join(right_df, on=left_df.id==right_df.id).drop(left_df.id)\n",
    "merged_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### union\n",
    "union all 不去重 union去重\n",
    "```SQL\n",
    "select\n",
    "    *\n",
    "from (\n",
    "    select\n",
    "        *\n",
    "    from up_df\n",
    "    union all\n",
    "    select \n",
    "        *\n",
    "    from down_df\n",
    ") t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_rows= 157\n",
      "down_rows= 87\n"
     ]
    }
   ],
   "source": [
    "up_df = all_df.where(F.col(\"sex\") == 'Male')\n",
    "down_df = all_df.where(F.col(\"sex\") != 'Male')\n",
    "print(\"up_rows=\", up_df.count())\n",
    "print(\"down_rows=\", down_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unioned= 244\n",
      "+----------+----+------+------+----+------+----+---+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size| id|\n",
      "+----------+----+------+------+----+------+----+---+\n",
      "|     41.19| 5.0|  Male|    No|Thur| Lunch|   5|142|\n",
      "|     20.69| 5.0|  Male|    No| Sun|Dinner|   5|185|\n",
      "|     30.46| 2.0|  Male|   Yes| Sun|Dinner|   5|187|\n",
      "|     28.15| 3.0|  Male|   Yes| Sat|Dinner|   5|216|\n",
      "|     29.85|5.14|Female|    No| Sun|Dinner|   5|155|\n",
      "+----------+----+------+------+----+------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union 是sql里面的union all\n",
    "unioned_df = up_df.union(down_df)\n",
    "print(\"unioned=\", unioned_df.count())\n",
    "unioned_df.where(F.col(\"size\") == 5).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf\n",
    "- 此处的udf指按行操作 即每一行产生一个结果 此处以 size * total_bill + 1 为例\n",
    "\n",
    "```SQL\n",
    "select\n",
    "    size * total_bill + 1  as res_col1\n",
    "    size + total_bill + 2  as res_col2\n",
    "from tips_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|          res_col1|res_col2|\n",
      "+------------------+--------+\n",
      "|             74.72|   24.43|\n",
      "|31.990000000000002|   15.33|\n",
      "+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 比较简单的运算可以直接使用列对象之间的运算就行\n",
    "res_df = spark_df.select((F.col(\"size\") * F.col(\"total_bill\") + 1).alias(\"res_col1\"), \n",
    "                (F.col(\"size\") + F.col(\"total_bill\") + 2).alias(\"res_col2\"))\n",
    "res_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|          res_col1|res_col2|\n",
      "+------------------+--------+\n",
      "|             74.72|   24.43|\n",
      "|31.990000000000002|   15.33|\n",
      "+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用udf 首先要使用 F.udf 创建一个函数对象 描述 一行的两个顺序的关系\n",
    "udf_func1 = F.udf(lambda a, b: a * b + 1)\n",
    "udf_func2 = F.udf(lambda a, b: a + b + 2)\n",
    "res_df = spark_df.select(udf_func1(F.col(\"size\"), F.col(\"total_bill\")).alias(\"res_col1\"), udf_func2(F.col(\"size\"), F.col(\"total_bill\")).alias(\"res_col2\"))\n",
    "res_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udaf\n",
    "```SQL\n",
    "select\n",
    "    avg(tip) + 100 as tip_avg,\n",
    "    avg(size) + 100 as size_avg\n",
    "from tips_df\n",
    "group by sex\n",
    "```\n",
    "- 此处的udaf指对列进行操作 每列产生一个结果 此处以 sum(size) + 1 为例\n",
    "\n",
    "PySpark 不支持UDAF   \n",
    "解决方法如下：\n",
    "collect_list + UDF = UDAF\n",
    "\n",
    "https://stackoverflow.com/questions/46187630/how-to-write-pyspark-udaf-on-multiple-columns   \n",
    "先使用 collect_set(col) and collect_list(col) 聚合成一个list 然后对list调用普通的 udf, udf的参数是一个list \n",
    "```SQL\n",
    "select\n",
    "     col2,\n",
    "     udf(collect_list(col1)) as res_col\n",
    "from table_name\n",
    "group by col2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|           tip_avg|          size_avg|\n",
      "+------------------+------------------+\n",
      "|102.99827868852459|102.56967213114754|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func_a(x):\n",
    "    return sum(x) / len(x) + 100\n",
    "\n",
    "udf_agg = F.udf(func_a)\n",
    "res_df = spark_df.agg(udf_agg(F.collect_list(F.col(\"tip\"))).alias(\"tip_avg\"), udf_agg(F.collect_list(F.col(\"size\"))).alias(\"size_avg\"))\n",
    "res_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
